import requests
from bs4 import BeautifulSoup
import json
import time
import logging
from datetime import datetime, timedelta
import os
import re
from typing import Dict, List, Optional

# Logging configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RojadirectaScraper:
    def __init__(self):
        self.base_url = "https://www.rojadirectaenvivo.pl"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': self.base_url
        })
    
    def get_daily_matches(self) -> List[Dict]:
        """GÃ¼nlÃ¼k maÃ§ programÄ±nÄ± Ã§eker"""
        try:
            logger.info("GÃ¼nlÃ¼k maÃ§lar Ã§ekiliyor...")
            response = self.session.get(self.base_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            matches = []
            
            # MaÃ§larÄ± bul - Rojadirecta'nÄ±n yapÄ±sÄ±na gÃ¶re
            match_elements = soup.find_all('div', class_=re.compile(r'match|partido|game', re.I))
            
            # EÄŸer Ã¶zel class yoksa, tablo veya liste elemanlarÄ±nÄ± ara
            if not match_elements:
                match_elements = soup.find_all('tr') + soup.find_all('li')
            
            for element in match_elements:
                match_text = element.get_text(strip=True)
                if any(league in match_text.upper() for league in ['SÃœPER LÄ°G', 'PREMIER LEAGUE', 'LA LIGA', 'SERIE A', 'BUNDESLIGA']):
                    match_data = self._parse_match_element(element)
                    if match_data:
                        matches.append(match_data)
            
            logger.info(f"{len(matches)} maÃ§ bulundu")
            return matches
            
        except Exception as e:
            logger.error(f"MaÃ§lar Ã§ekilirken hata: {str(e)}")
            return []
    
    def _parse_match_element(self, element) -> Optional[Dict]:
        """MaÃ§ elementini parse eder"""
        try:
            link_element = element.find('a')
            if not link_element:
                return None
            
            match_name = link_element.get_text(strip=True)
            match_url = link_element.get('href')
            
            if match_url and not match_url.startswith('http'):
                match_url = self.base_url + match_url if match_url.startswith('/') else f"{self.base_url}/{match_url}"
            
            # MaÃ§ saatini bul
            time_pattern = r'\d{1,2}:\d{2}'
            time_match = re.search(time_pattern, element.get_text())
            match_time = time_match.group() if time_match else "Bilinmiyor"
            
            return {
                'name': match_name,
                'url': match_url,
                'time': match_time,
                'league': self._detect_league(match_name),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"MaÃ§ parse edilirken hata: {str(e)}")
            return None
    
    def _detect_league(self, match_name: str) -> str:
        """MaÃ§Ä±n hangi ligde olduÄŸunu tespit eder"""
        leagues = {
            'SÃœPER LÄ°G': ['sÃ¼per lig', 'super lig', 'superlig'],
            'PREMIER LEAGUE': ['premier league', 'premier'],
            'LA LIGA': ['la liga', 'laliga'],
            'SERIE A': ['serie a', 'seriea'],
            'BUNDESLIGA': ['bundesliga'],
            'LIGUE 1': ['ligue 1', 'ligue1']
        }
        
        match_name_upper = match_name.upper()
        for league, keywords in leagues.items():
            if any(keyword.upper() in match_name_upper for keyword in keywords):
                return league
        
        return "DiÄŸer"
    
    def get_stream_links(self, match_url: str) -> List[Dict]:
        """MaÃ§ sayfasÄ±ndaki yayÄ±n linklerini Ã§eker"""
        try:
            logger.info(f"YayÄ±n linkleri Ã§ekiliyor: {match_url}")
            response = self.session.get(match_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            stream_links = []
            
            # Stream linklerini bul
            stream_elements = soup.find_all('a', href=re.compile(r'stream|yayÄ±n|canlÄ±|live', re.I))
            
            for stream_element in stream_elements:
                stream_url = stream_element.get('href')
                stream_name = stream_element.get_text(strip=True)
                
                if stream_url and self._is_valid_stream_url(stream_url):
                    stream_links.append({
                        'name': stream_name or 'Bilinmeyen YayÄ±n',
                        'url': stream_url,
                        'quality': self._detect_quality(stream_name),
                        'working': True  # Test edilecek
                    })
            
            # Benzersiz linkler
            unique_links = []
            seen_urls = set()
            for link in stream_links:
                if link['url'] not in seen_urls:
                    unique_links.append(link)
                    seen_urls.add(link['url'])
            
            logger.info(f"{len(unique_links)} yayÄ±n linki bulundu")
            return unique_links[:10]  # Ä°lk 10 link
            
        except Exception as e:
            logger.error(f"YayÄ±n linkleri Ã§ekilirken hata: {str(e)}")
            return []
    
    def _is_valid_stream_url(self, url: str) -> bool:
        """GeÃ§erli bir stream URL'si mi kontrol eder"""
        invalid_patterns = [
            r'facebook', r'twitter', r'instagram', r'telegram',
            r'mailto:', r'tel:', r'javascript:',
            r'\.pdf$', r'\.doc$', r'\.xls$'
        ]
        
        url_lower = url.lower()
        return (url.startswith('http') and 
                not any(pattern in url_lower for pattern in invalid_patterns))
    
    def _detect_quality(self, stream_name: str) -> str:
        """YayÄ±n kalitesini tespit eder"""
        quality_patterns = {
            'HD': [r'hd', r'high definition', r'1080p', r'720p'],
            'SD': [r'sd', r'standard', r'480p', r'360p'],
            '4K': [r'4k', r'ultra hd', r'2160p']
        }
        
        stream_name_lower = stream_name.lower()
        for quality, patterns in quality_patterns.items():
            if any(re.search(pattern, stream_name_lower) for pattern in patterns):
                return quality
        
        return 'Bilinmiyor'
    
    def test_stream_link(self, stream_url: str) -> bool:
        """YayÄ±n linkinin Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± test eder"""
        try:
            response = self.session.head(stream_url, timeout=10, allow_redirects=True)
            return response.status_code == 200
        except:
            return False

class StreamManager:
    def __init__(self):
        self.scraper = RojadirectaScraper()
        self.data_file = 'streams_data.json'
    
    def get_daily_streams(self) -> Dict:
        """GÃ¼nlÃ¼k maÃ§ ve yayÄ±n bilgilerini getirir"""
        matches = self.scraper.get_daily_matches()
        results = {}
        
        for match in matches[:5]:  # Ä°lk 5 maÃ§ iÃ§in
            logger.info(f"MaÃ§ iÅŸleniyor: {match['name']}")
            stream_links = self.scraper.get_stream_links(match['url'])
            
            # Stream linklerini test et
            for stream_link in stream_links:
                stream_link['working'] = self.scraper.test_stream_link(stream_link['url'])
            
            results[match['name']] = {
                'match_info': match,
                'streams': stream_links,
                'last_updated': datetime.now().isoformat()
            }
            
            time.sleep(2)  # Rate limiting
        
        self._save_data(results)
        return results
    
    def _save_data(self, data: Dict):
        """Veriyi JSON dosyasÄ±na kaydeder"""
        try:
            with open(self.data_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info("Veri kaydedildi")
        except Exception as e:
            logger.error(f"Veri kaydedilirken hata: {str(e)}")

def main():
    """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu"""
    manager = StreamManager()
    
    print("ğŸš€ Rojadirecta Stream Scraper BaÅŸlatÄ±lÄ±yor...")
    print("=" * 50)
    
    try:
        results = manager.get_daily_streams()
        
        print(f"\nğŸ“Š Toplam {len(results)} maÃ§ bulundu:")
        print("=" * 50)
        
        for match_name, data in results.items():
            working_streams = [s for s in data['streams'] if s['working']]
            print(f"\nâš½ {match_name}")
            print(f"ğŸ•’ Saat: {data['match_info']['time']}")
            print(f"ğŸ“º Ã‡alÄ±ÅŸan YayÄ±nlar: {len(working_streams)}/{len(data['streams'])}")
            
            for stream in working_streams[:3]:  # Ä°lk 3 Ã§alÄ±ÅŸan yayÄ±n
                print(f"   ğŸ”— {stream['name']} - {stream['quality']}")
        
        print(f"\nâœ… Veriler '{manager.data_file}' dosyasÄ±na kaydedildi")
        
    except Exception as e:
        logger.error(f"Ana fonksiyonda hata: {str(e)}")
        print("âŒ Bir hata oluÅŸtu, detaylar iÃ§in loglarÄ± kontrol edin")

if __name__ == "__main__":
    main()