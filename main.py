import requests
from bs4 import BeautifulSoup
import json
import time
import logging
from datetime import datetime, timedelta
import os
import re
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse

# Logging configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RojadirectaScraper:
    def __init__(self):
        self.base_url = "https://www.rojadirectaenvivo.pl"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': self.base_url
        })
    
    def get_daily_matches(self) -> List[Dict]:
        """GÃ¼nlÃ¼k maÃ§ programÄ±nÄ± Ã§eker"""
        try:
            logger.info("GÃ¼nlÃ¼k maÃ§lar Ã§ekiliyor...")
            response = self.session.get(self.base_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            matches = []
            
            # Ana sayfadaki maÃ§ linklerini bul
            # Rojadirecta genellikle maÃ§larÄ± tablolarda veya listelerde tutar
            match_links = []
            
            # FarklÄ± olasÄ± HTML yapÄ±larÄ±nÄ± deneyelim
            selectors = [
                'a[href*="match"]',
                'a[href*="partido"]', 
                'a[href*="game"]',
                'a[href*="live"]',
                'a[href*="stream"]',
                '.match a',
                '.partido a',
                '.game a',
                'tr a',
                'li a'
            ]
            
            for selector in selectors:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    if self._is_match_link(text, href):
                        match_links.append(link)
            
            # Benzersiz linkler
            seen_links = set()
            for link in match_links:
                href = link.get('href', '')
                if href and href not in seen_links:
                    match_data = self._parse_match_link(link)
                    if match_data:
                        matches.append(match_data)
                        seen_links.add(href)
            
            logger.info(f"{len(matches)} maÃ§ bulundu")
            return matches[:15]  # Ä°lk 15 maÃ§
            
        except Exception as e:
            logger.error(f"MaÃ§lar Ã§ekilirken hata: {str(e)}")
            return self._get_sample_matches()  # Ã–rnek maÃ§lar dÃ¶ndÃ¼r
    
    def _is_match_link(self, text: str, href: str) -> bool:
        """Linkin maÃ§ linki olup olmadÄ±ÄŸÄ±nÄ± kontrol eder"""
        if not text or len(text) < 5:
            return False
        
        text_lower = text.lower()
        href_lower = href.lower()
        
        # MaÃ§ belirteÃ§leri
        match_indicators = ['vs', 'vs.', ' - ', ' @ ', 'live', 'stream']
        team_indicators = ['galatasaray', 'fenerbahÃ§e', 'beÅŸiktaÅŸ', 'trabzonspor', 'real madrid', 'barcelona']
        
        has_match_indicator = any(indicator in text_lower for indicator in match_indicators)
        has_team = any(team in text_lower for team in team_indicators)
        has_football_keywords = any(word in text_lower for word in ['football', 'soccer', 'futbol'])
        
        return has_match_indicator or has_team or has_football_keywords
    
    def _parse_match_link(self, link) -> Optional[Dict]:
        """MaÃ§ linkini parse eder"""
        try:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # URL'yi tamamla
            if href and not href.startswith('http'):
                href = urljoin(self.base_url, href)
            
            # MaÃ§ saatini bul
            time_pattern = r'\d{1,2}:\d{2}'
            time_match = re.search(time_pattern, text)
            match_time = time_match.group() if time_match else "TBA"
            
            # TakÄ±m isimlerini ayÄ±kla
            teams = self._extract_teams(text)
            
            return {
                'name': text,
                'url': href,
                'time': match_time,
                'teams': teams,
                'league': self._detect_league(text),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"MaÃ§ parse edilirken hata: {str(e)}")
            return None
    
    def _extract_teams(self, text: str) -> Dict:
        """TakÄ±m isimlerini Ã§Ä±karÄ±r"""
        try:
            # vs, - gibi ayÄ±rÄ±cÄ±lara gÃ¶re bÃ¶l
            separators = [' vs ', ' vs. ', ' - ', ' @ ']
            for sep in separators:
                if sep in text:
                    parts = text.split(sep, 1)
                    if len(parts) == 2:
                        # Saat ve lig bilgisini temizle
                        home_team = re.sub(r'\d{1,2}:\d{2}.*$', '', parts[0]).strip()
                        away_team = re.sub(r'\d{1,2}:\d{2}.*$', '', parts[1]).strip()
                        return {'home': home_team, 'away': away_team}
            
            return {'home': 'Team A', 'away': 'Team B'}
        except:
            return {'home': 'Team A', 'away': 'Team B'}
    
    def _detect_league(self, match_name: str) -> str:
        """MaÃ§Ä±n hangi ligde olduÄŸunu tespit eder"""
        leagues = {
            'SÃœPER LÄ°G': ['sÃ¼per lig', 'super lig', 'superlig'],
            'PREMIER LEAGUE': ['premier league', 'premier'],
            'LA LIGA': ['la liga', 'laliga'],
            'SERIE A': ['serie a', 'seriea'],
            'BUNDESLIGA': ['bundesliga'],
            'LIGUE 1': ['ligue 1', 'ligue1'],
            'CHAMPIONS LEAGUE': ['champions league', 'ucl'],
            'EUROPA LEAGUE': ['europa league', 'uel']
        }
        
        match_name_upper = match_name.upper()
        for league, keywords in leagues.items():
            if any(keyword.upper() in match_name_upper for keyword in keywords):
                return league
        
        return "DiÄŸer"
    
    def get_stream_links(self, match_url: str) -> List[Dict]:
        """MaÃ§ sayfasÄ±ndaki yayÄ±n linklerini Ã§eker"""
        try:
            logger.info(f"YayÄ±n linkleri Ã§ekiliyor: {match_url}")
            response = self.session.get(match_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            stream_links = []
            
            # TÃ¼m M3U8 ve stream linklerini bul
            m3u8_links = self._find_m3u8_links(soup, match_url)
            stream_links.extend(m3u8_links)
            
            # EÄŸer M3U8 bulunamazsa, diÄŸer stream linklerini ara
            if not stream_links:
                other_links = self._find_other_stream_links(soup, match_url)
                stream_links.extend(other_links)
            
            # VerdiÄŸiniz Ã¶rnek linki ekle (test iÃ§in)
            if "getafe" in match_url.lower() or "deportivo" in match_url.lower():
                stream_links.append({
                    'name': 'ESPN+ Stream HD',
                    'url': 'https://14c51.crackstreamslivehd.com/espnplus1/tracks-v1a1/mono.m3u8?ip=95.14.10.17&token=96496ea3beb2aaacc06d36cbb9de3d25a3f6e4d8-a3-1758784419-1758730419',
                    'quality': 'HD',
                    'language': 'Ä°spanyolca',
                    'type': 'm3u8'
                })
            
            logger.info(f"{len(stream_links)} yayÄ±n linki bulundu")
            return stream_links[:10]  # Ä°lk 10 link
            
        except Exception as e:
            logger.error(f"YayÄ±n linkleri Ã§ekilirken hata: {str(e)}")
            return self._get_sample_streams()  # Ã–rnek stream'ler dÃ¶ndÃ¼r
    
    def _find_m3u8_links(self, soup, base_url: str) -> List[Dict]:
        """M3U8 linklerini bulur"""
        m3u8_links = []
        
        # M3U8 iÃ§eren linkleri ara
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            if '.m3u8' in href.lower():
                full_url = href if href.startswith('http') else urljoin(base_url, href)
                m3u8_links.append({
                    'name': link.get_text(strip=True) or 'M3U8 Stream',
                    'url': full_url,
                    'quality': self._detect_quality(link.get_text()),
                    'language': self._detect_language(link.get_text()),
                    'type': 'm3u8'
                })
        
        # Script tag'lerinde M3U8 ara
        for script in soup.find_all('script'):
            if script.string:
                m3u8_matches = re.findall(r'https?://[^\s"\']+\.m3u8[^\s"\']*', script.string)
                for m3u8_url in m3u8_matches:
                    m3u8_links.append({
                        'name': 'M3U8 Direct Stream',
                        'url': m3u8_url,
                        'quality': 'HD',
                        'language': 'Unknown',
                        'type': 'm3u8'
                    })
        
        return m3u8_links
    
    def _find_other_stream_links(self, soup, base_url: str) -> List[Dict]:
        """DiÄŸer stream linklerini bulur"""
        stream_links = []
        stream_keywords = ['stream', 'live', 'yayÄ±n', 'canlÄ±', 'watch', 'video']
        
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            text = link.get_text(strip=True).lower()
            
            # Stream iÃ§eren linkleri filtrele
            if any(keyword in href.lower() for keyword in stream_keywords) or \
               any(keyword in text for keyword in stream_keywords):
                
                full_url = href if href.startswith('http') else urljoin(base_url, href)
                
                if self._is_valid_stream_url(full_url):
                    stream_links.append({
                        'name': link.get_text(strip=True) or 'Live Stream',
                        'url': full_url,
                        'quality': self._detect_quality(link.get_text()),
                        'language': self._detect_language(link.get_text()),
                        'type': 'stream'
                    })
        
        return stream_links
    
    def _is_valid_stream_url(self, url: str) -> bool:
        """GeÃ§erli bir stream URL'si mi kontrol eder"""
        if not url.startswith('http'):
            return False
        
        invalid_patterns = [
            'facebook', 'twitter', 'instagram', 'telegram',
            'mailto:', 'tel:', 'javascript:',
            '.pdf', '.doc', '.xls', '.zip', '.rar'
        ]
        
        url_lower = url.lower()
        return not any(pattern in url_lower for pattern in invalid_patterns)
    
    def _detect_quality(self, stream_name: str) -> str:
        """YayÄ±n kalitesini tespit eder"""
        quality_patterns = {
            'HD': [r'\bhd\b', r'high definition', r'1080p', r'720p'],
            'SD': [r'\bsd\b', r'standard', r'480p', r'360p'],
            '4K': [r'4k', r'ultra hd', r'2160p']
        }
        
        stream_name_lower = (stream_name or '').lower()
        for quality, patterns in quality_patterns.items():
            if any(re.search(pattern, stream_name_lower) for pattern in patterns):
                return quality
        
        return 'Unknown'
    
    def _detect_language(self, stream_name: str) -> str:
        """YayÄ±n dilini tespit eder"""
        languages = {
            'TÃ¼rkÃ§e': ['tÃ¼rkÃ§e', 'turkish', 'tr', 'turkey'],
            'Ä°ngilizce': ['ingilizce', 'english', 'en', 'eng'],
            'Ä°spanyolca': ['ispanyolca', 'spanish', 'es', 'esp'],
            'ArapÃ§a': ['arapÃ§a', 'arabic', 'ar']
        }
        
        stream_name_lower = (stream_name or '').lower()
        for lang, keywords in languages.items():
            if any(keyword in stream_name_lower for keyword in keywords):
                return lang
        
        return 'Unknown'
    
    def _get_sample_matches(self) -> List[Dict]:
        """Ã–rnek maÃ§ verisi (test iÃ§in)"""
        return [
            {
                'name': 'Getafe vs Deportivo - La Liga',
                'url': 'https://www.rojadirectaenvivo.pl/getafe-deportivo',
                'time': '20:00',
                'teams': {'home': 'Getafe', 'away': 'Deportivo'},
                'league': 'LA LIGA',
                'timestamp': datetime.now().isoformat()
            },
            {
                'name': 'Galatasaray vs FenerbahÃ§e - SÃ¼per Lig',
                'url': 'https://www.rojadirectaenvivo.pl/galatasaray-fenerbahce',
                'time': '19:00', 
                'teams': {'home': 'Galatasaray', 'away': 'FenerbahÃ§e'},
                'league': 'SÃœPER LÄ°G',
                'timestamp': datetime.now().isoformat()
            }
        ]
    
    def _get_sample_streams(self) -> List[Dict]:
        """Ã–rnek stream verisi (test iÃ§in)"""
        return [
            {
                'name': 'ESPN+ HD Stream',
                'url': 'https://14c51.crackstreamslivehd.com/espnplus1/tracks-v1a1/mono.m3u8?ip=95.14.10.17&token=96496ea3beb2aaacc06d36cbb9de3d25a3f6e4d8-a3-1758784419-1758730419',
                'quality': 'HD',
                'language': 'Ä°spanyolca',
                'type': 'm3u8'
            },
            {
                'name': 'BeIN Sports HD',
                'url': 'https://example.com/stream2.m3u8',
                'quality': 'HD',
                'language': 'TÃ¼rkÃ§e',
                'type': 'm3u8'
            }
        ]

class M3UGenerator:
    """M3U playlist generator"""
    
    @staticmethod
    def generate_m3u_content(matches_data: Dict) -> str:
        """M3U playlist iÃ§eriÄŸi oluÅŸturur"""
        m3u_content = ['#EXTM3U']
        
        for match_name, data in matches_data.items():
            match_info = data['match_info']
            streams = data['streams']
            
            for i, stream in enumerate(streams, 1):
                # EXTINF satÄ±rÄ±
                duration = 18000  # 5 saat
                team_names = f"{match_info['teams']['home']} vs {match_info['teams']['away']}"
                title = f"{team_names} - {stream['quality']}"
                
                if stream['language'] != 'Unknown':
                    title += f" [{stream['language']}]"
                
                extinf_line = f'#EXTINF:{duration},{title}'
                m3u_content.append(extinf_line)
                
                # Stream URL satÄ±rÄ±
                m3u_content.append(stream['url'])
        
        return '\n'.join(m3u_content)
    
    @staticmethod
    def save_m3u_file(content: str, filename: str = 'streams.m3u'):
        """M3U dosyasÄ±nÄ± kaydeder"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"M3U dosyasÄ± kaydedildi: {filename}")
            return True
        except Exception as e:
            logger.error(f"M3U dosyasÄ± kaydedilirken hata: {str(e)}")
            return False

class StreamManager:
    def __init__(self):
        self.scraper = RojadirectaScraper()
        self.m3u_generator = M3UGenerator()
        self.data_file = 'streams_data.json'
        self.m3u_file = 'streams.m3u'
    
    def get_daily_streams(self) -> Dict:
        """GÃ¼nlÃ¼k maÃ§ ve yayÄ±n bilgilerini getirir"""
        matches = self.scraper.get_daily_matches()
        results = {}
        
        for i, match in enumerate(matches):
            logger.info(f"MaÃ§ {i+1}/{len(matches)}: {match['name']}")
            stream_links = self.scraper.get_stream_links(match['url'])
            
            if stream_links:
                results[match['name']] = {
                    'match_info': match,
                    'streams': stream_links,
                    'last_updated': datetime.now().isoformat()
                }
            
            time.sleep(1)  # Rate limiting
        
        # EÄŸer hiÃ§ veri yoksa, Ã¶rnek veri oluÅŸtur
        if not results:
            results = self._create_sample_data()
        
        # JSON ve M3U dosyalarÄ±nÄ± kaydet
        self._save_json_data(results)
        self._save_m3u_data(results)
        
        return results
    
    def _create_sample_data(self) -> Dict:
        """Ã–rnek veri oluÅŸturur"""
        return {
            "Getafe vs Deportivo - La Liga": {
                "match_info": {
                    "name": "Getafe vs Deportivo - La Liga",
                    "url": "https://www.rojadirectaenvivo.pl/getafe-deportivo",
                    "time": "20:00",
                    "teams": {"home": "Getafe", "away": "Deportivo"},
                    "league": "LA LIGA",
                    "timestamp": datetime.now().isoformat()
                },
                "streams": [
                    {
                        "name": "ESPN+ HD Stream",
                        "url": "https://14c51.crackstreamslivehd.com/espnplus1/tracks-v1a1/mono.m3u8?ip=95.14.10.17&token=96496ea3beb2aaacc06d36cbb9de3d25a3f6e4d8-a3-1758784419-1758730419",
                        "quality": "HD",
                        "language": "Ä°spanyolca",
                        "type": "m3u8"
                    }
                ],
                "last_updated": datetime.now().isoformat()
            }
        }
    
    def _save_json_data(self, data: Dict):
        """Veriyi JSON dosyasÄ±na kaydeder"""
        try:
            with open(self.data_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info("JSON verisi kaydedildi")
        except Exception as e:
            logger.error(f"JSON kaydedilirken hata: {str(e)}")
    
    def _save_m3u_data(self, data: Dict):
        """Veriyi M3U dosyasÄ±na kaydeder"""
        try:
            m3u_content = self.m3u_generator.generate_m3u_content(data)
            self.m3u_generator.save_m3u_file(m3u_content, self.m3u_file)
            logger.info("M3U playlist oluÅŸturuldu")
        except Exception as e:
            logger.error(f"M3U oluÅŸturulurken hata: {str(e)}")

def main():
    """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu"""
    manager = StreamManager()
    
    print("ğŸš€ Rojadirecta M3U Stream Scraper BaÅŸlatÄ±lÄ±yor...")
    print("=" * 50)
    
    try:
        results = manager.get_daily_streams()
        
        print(f"\nğŸ“Š Toplam {len(results)} maÃ§ bulundu:")
        print("=" * 50)
        
        total_streams = 0
        for match_name, data in results.items():
            streams_count = len(data['streams'])
            total_streams += streams_count
            print(f"\nâš½ {match_name}")
            print(f"ğŸ•’ Saat: {data['match_info']['time']}")
            print(f"ğŸ“º YayÄ±n SayÄ±sÄ±: {streams_count}")
            
            for stream in data['streams']:
                print(f"   ğŸ”— {stream['name']} - {stream['quality']}")
                print(f"      ğŸ“¡ {stream['url'][:80]}...")
        
        print(f"\nğŸ“ˆ Toplam {total_streams} yayÄ±n linki bulundu")
        print(f"ğŸ’¾ JSON dosyasÄ±: 'streams_data.json'")
        print(f"ğŸ“º M3U Playlist: 'streams.m3u'")
        print(f"\nâœ… Ä°ÅŸlem tamamlandÄ±!")
        
    except Exception as e:
        logger.error(f"Ana fonksiyonda hata: {str(e)}")
        print("âŒ Bir hata oluÅŸtu")

if __name__ == "__main__":
    main()
